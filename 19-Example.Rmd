# Example

We perform all the data analysis on our own metabolomic data in this chapter. There are several datasets: 

- **GvHD stool metabolites TM wide-target sequencing**: /home/xuxiaomin/project/NanFangHospitalGvHD/00.data/metabolites_TM/MWY-20-522-01_2021-03-25_17-34-06/1.Data_assess/ALL_sample_data.xlsx

- **GvHD stool metabolites SCFA**: /home/xuxiaomin/project/NanFangHospitalGvHD/00.data/metabolites_scfa/SCFA.levels.xlsx

- **PD-1 mice trial (round 2)  serum metabolites TM wide-target sequencing**: /home/xuxiaomin/project/pd1_mice/Round_2/00.data/MetaboliteSerum/MWY-20-049/1.Data_assess/all_group/ALL_sample_data.xlsx

and the metadata: 

- **GvHD metadata**: /home/xuxiaomin/project/NanFangHospitalGvHD/00.data/metadata/metadata_v4.txt


We transform their names as following:

- GvHD_stool_metabolites_TM.xlsx

- GvHD_stool_metabolites_SCFA.xlsx        

- PD1_mice_serum_metabolites_TM.xlsx

- GvHD_metadata.txt

Here, we use the `GvHD_stool_metabolites_TM.xlsx` to practice our template.


## Loading packages
```{r}
knitr::opts_chunk$set(warning = F)

library(dplyr)
library(tibble)
library(POMA)
library(ggplot2)
library(ggraph)
library(plotly)
library(SummarizedExperiment)
library(readxl)

library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms

library(NetCoMi)
library(SPRING)
library(SpiecEasi)

library(WGCNA)


# rm(list = ls())
options(stringsAsFactors = F)
options(future.globals.maxSize = 1000 * 1024^2)
```


## Importing data

* features table
```{r}
profile <- readxl::read_xlsx("./dataset/GvHD_stool_metabolites_TM.xlsx", sheet = 1)
head(profile)
```

* metadata table
```{r}
metadata <- data.table::fread("./dataset/GvHD_metadata.txt")
head(metadata)
```

* Data Preparation: SummarizedExperiment object

-  Renaming column

-  Replace 9 by NA

-  column with `mix` prefix are regards as **QC** samples

```{r}
getSEobject <- function(x, y) {
  
  # x = metadata
  # y = profile

  # mix : qc samples
  qc_samples <- grep("mix", colnames(profile), value = T)
  qc_groups <- data.frame(SampleID = qc_samples,
                          V1_outcome = rep("QC", length(qc_samples)),
                          SampleName = rep("SampleName", length(qc_samples)),
                          FMT_status = rep("FMT_status", length(qc_samples)),
                          gender = rep("gender", length(qc_samples)),
                          age = rep("age", length(qc_samples)))
  # target table
  target <- x %>% 
    dplyr::filter(FMT_status != "") %>%
    dplyr::mutate(SampleID = paste(SampleName, FMT_status, sep = "_")) %>%
    dplyr::select(SampleID, V1_outcome, SampleName, FMT_status, gender, age) %>%
    rbind(qc_groups)
  
  # profile column 
  colnames(y) <- gsub("-", "_", colnames(y))

  sid <- intersect(target$SampleID, colnames(y))
  
  features <- y %>% 
    dplyr::select(all_of(sid)) %>%
    data.frame() %>% t()
  colnames(features) <- y$Index
  
  # replace 9 by NA 
  features[features == 9] <- NA
  
  target <- target[pmatch(sid, target$SampleID), , F]
  
  res <- PomaSummarizedExperiment(target = target, 
                                  features = features)
  return(res)  
}

se <- getSEobject(metadata, profile)

se
```

## Data Processing

### Data Checking

Features in PomaSummarizedExperiment object must have the following criterion:

* All data values are numeric.

* A total of 0 (0%) missing values were detected.
```{r}
CheckData <- function(object) {
  
  features_tab <- SummarizedExperiment::assay(object)
  
  # numeric & missing values
  int_mat <- features_tab
  rowNms <- rownames(int_mat)
  colNms <- colnames(int_mat)
  naNms <- sum(is.na(int_mat))
  for (i in 1:ncol(int_mat)) {
    if (class(int_mat[, i]) == "integer64") {
      int_mat[, i] <- as.double(int_mat[, i])
    }
  }
  
  num_mat <- apply(int_mat, 2, as.numeric)
  if (sum(is.na(num_mat)) > naNms) {
    num_mat <- apply(int_mat, 2, function(x) as.numeric(gsub(",",  "", x)))
    if (sum(is.na(num_mat)) > naNms) {
      message("<font color=\"red\">Non-numeric values were found and replaced by NA.</font>")
    } else {
      message("All data values are numeric.")
    }
  } else {
    message("All data values are numeric.")
  }
  
  int_mat <- num_mat
  rownames(int_mat) <- rowNms
  colnames(int_mat) <- colNms
  varCol <- apply(int_mat, 2, var, na.rm = T)
  constCol <- (varCol == 0 | is.na(varCol))
  constNum <- sum(constCol, na.rm = T)
  if (constNum > 0) {
    message(paste("<font color=\"red\">", constNum, 
      "features with a constant or single value across samples were found and deleted.</font>"))
    int_mat <- int_mat[, !constCol, drop = FALSE]
  }
  
  totalCount <- nrow(int_mat) * ncol(int_mat)
  naCount <- sum(is.na(int_mat))
  naPercent <- round(100 * naCount/totalCount, 1)

  message(paste("A total of ", naCount, " (", naPercent, 
    "%) missing values were detected.", sep = ""))  
  
  # save int_mat into se object 
  target <- SummarizedExperiment::colData(object) %>%
    data.frame() %>%
    tibble::rownames_to_column("SampleID")
  res <- PomaSummarizedExperiment(target = target, 
                                  features = t(int_mat))
  
  return(res)
}

se <- CheckData(object = se)
se
```

### Data Filtering
```{r}
FilterFeature <- function(
                       object,
                       qc_label,
                       method = c("none", "iqr", "rsd", 
                                  "nrsd", "mean", "sd",
                                  "mad", "median"),
                       rsd_cutoff = 25) {
  
  features_tab <- SummarizedExperiment::assay(object) 
  metadata_tab <- SummarizedExperiment::colData(object) 
  
  # QC samples
  qc_samples <- metadata_tab %>% data.frame() %>%
    dplyr::filter(group == qc_label)
  if (dim(qc_samples)[1] == 0) {
    stop("No qc samples have been chosen, please check your input")
  }
  
  # QC samples' feature table
  qc_feature <- features_tab[, colnames(features_tab) %in% rownames(qc_samples)] %>%
    t()
  
  # filter features by QC RSD
  rsd <- rsd_cutoff / 100
  sds <- apply(qc_feature, 2, sd, na.rm = T)
  mns <- apply(qc_feature, 2, mean, na.rm = T)
  rsd_vals <- abs(sds/mns) %>% na.omit()
  gd_inx <- rsd_vals < rsd
  int_mat <- features_tab[gd_inx, ]
  message("Removed ", (dim(qc_feature)[2] - dim(int_mat)[1]), 
  " features based on QC RSD values. QC samples are excluded from downstream functional analysis.")
  
  # whether to filter features by percentage according to the number
  PerformFeatureFilter <- function(datMatrix, 
                                   qc_method = method,
                                   remain_num = NULL) {
    
    dat <- datMatrix
    feat_num <- ncol(dat)
    feat_nms <- colnames(dat)
    nm <- NULL
    if (qc_method == "none" && feat_num < 5000) { # only allow for less than 4000
      remain <- rep(TRUE, feat_num)
      nm <- "No filtering was applied"
    } else {
      if (qc_method == "rsd"){
        sds <- apply(dat, 2, sd, na.rm = T)
        mns <- apply(dat, 2, mean, na.rm = T)
        filter_val <- abs(sds/mns)
        nm <- "Relative standard deviation"
      } else if (qc_method == "nrsd" ) {
        mads <- apply(dat, 2, mad, na.rm = T)
        meds <- apply(dat, 2, median, na.rm = T)
        filter_val <- abs(mads/meds)
        nm <- "Non-paramatric relative standard deviation"
      } else if (qc_method == "mean") {
        filter_val <- apply(dat, 2, mean, na.rm = T)
        nm <- "mean"
      } else if (qc_method == "sd") {
        filter_val <- apply(dat, 2, sd, na.rm = T)
        nm <- "standard deviation"
      } else if (qc_method == "mad") {
        filter_val <- apply(dat, 2, mad, na.rm = T)
        nm <- "Median absolute deviation"
      } else if (qc_method == "median") {
        filter_val <- apply(dat, 2, median, na.rm = T)
        nm <- "median"
      } else if (qc_method == "iqr") { # iqr
        filter_val <- apply(dat, 2, IQR, na.rm = T)
        nm <- "Interquantile Range"
      }
      
      # get the rank of the filtered variables
      rk <- rank(-filter_val, ties.method = "random")
      
      if (is.null(remain_num)) { # apply empirical filtering based on data size
          if (feat_num < 250) { # reduce 5%
            remain <- rk < feat_num * 0.95
            message("Further feature filtering based on ", nm)
          } else if (feat_num < 500) { # reduce 10%
            remain <- rk < feat_num * 0.9
            message("Further feature filtering based on ", nm)
          } else if (feat_num < 1000) { # reduce 25%
            remain <- rk < feat_num * 0.75
            message("Further feature filtering based on ", nm)
          } else { # reduce 40%, if still over 5000, then only use top 5000
            remain <- rk < feat_num * 0.6
            message("Further feature filtering based on ", nm)
          }
      } else {
        remain <- rk < remain_num
      }
    }
    
    res <- datMatrix[, remain]
    
    return(res)
  }  
  
  feature_res <- PerformFeatureFilter(t(int_mat))
  
  # remove QC samples 
  feature_final <- feature_res[!rownames(feature_res) %in% rownames(qc_samples), ]
  
  # save int_mat into se object 
  target <- metadata_tab %>% 
    data.frame() %>%
    tibble::rownames_to_column("SampleID") %>%
    dplyr::filter(SampleID %in% rownames(feature_final))
  res <- PomaSummarizedExperiment(target = target, 
                                  features = feature_final)
  
  return(res) 
}

se_filter <- FilterFeature(object = se,
                           qc_label = "QC",
                           method = "iqr")
se_filter
```

### Missing Value Imputation
```{r}
se_impute <- PomaImpute(
                    se_filter, 
                    ZerosAsNA = TRUE, 
                    RemoveNA = TRUE, 
                    cutoff = 20, 
                    method = "knn")
se_impute
```

### Data Normalization

#### Normalization by **NormalizeData** function
```{r}
NormalizeData <- function(
                      object,
                      rowNorm = c("Quantile", "GroupPQN", "SamplePQN", 
                                  "CompNorm", "SumNorm", "MedianNorm",
                                  "SpecNorm", "None"),
                      transNorm = c("LogNorm", "SrNorm", "CrNorm", "None"),
                      scaleNorm = c("MeanCenter", "AutoNorm", "ParetoNorm", 
                                    "RangeNorm", "None"),
                      ref = NULL,
                      SpeWeight = 1) {
  
  features_tab <- SummarizedExperiment::assay(object) 
  metadata_tab <- SummarizedExperiment::colData(object)   
  
  data <- t(features_tab)
  colNames <- colnames(data)
  rowNames <- rownames(data)
  
  #############################################
  # Sample normalization
  # perform quantile normalization on the raw data (can be log transformed later by user)
  QuantileNormalize <- function(data) {
    return(t(preprocessCore::normalize.quantiles(t(data), copy=FALSE)));
  }
  # normalize by a reference sample (probability quotient normalization)
  # ref should be the name of the reference sample
  ProbNorm <- function(x, ref_smpl) {
    return(x/median(as.numeric(x/ref_smpl), na.rm = T))
  }
  
  # normalize by a reference reference (i.e. creatinine)
  # ref should be the name of the cmpd
  CompNorm <- function(x, ref) {
    return(1000*x/x[ref])
  }
  
  SumNorm <- function(x) {
    return(1000*x/sum(x, na.rm = T))
  }
  
  # normalize by median
  MedianNorm <- function(x) {
    return(x/median(x, na.rm = T))
  }  
  
  # row-wise normalization
  if (rowNorm == "Quantile") {
    data <- QuantileNormalize(data)
    # this can introduce constant variables if a variable is 
    # at the same rank across all samples (replaced by its average across all)
    varCol <- apply(data, 2, var, na.rm = T)
    constCol <- (varCol == 0 | is.na(varCol))
    constNum <- sum(constCol, na.rm = T)
    if (constNum > 0) {
      message(paste("After quantile normalization", constNum, 
                    "features with a constant value were found and deleted."))
      data <- data[, !constCol, drop = FALSE]
      colNames <- colnames(data)
      rowNames <- rownames(data)
    }
    rownm <- "Quantile Normalization"
  } else if (rowNorm == "GroupPQN") {
    grp_inx <- metadata_tab$group == ref
    ref.smpl <- apply(data[grp_inx, , drop = FALSE], 2, mean)
    data <- t(apply(data, 1, ProbNorm, ref.smpl))
    rownm <- "Probabilistic Quotient Normalization by a reference group"
  } else if (rowNorm == "SamplePQN") {
    ref.smpl <- data[ref, , drop = FALSE]
    data <- t(apply(data, 1, ProbNorm, ref.smpl))
    rownm <- "Probabilistic Quotient Normalization by a reference sample"
  } else if (rowNorm == "CompNorm") {
    data <- t(apply(t(data), 1, CompNorm, ref))
    rownm <- "Normalization by a reference feature";
  } else if (rowNorm == "SumNorm") {
    data <- t(apply(data, 1, SumNorm))
    rownm <- "Normalization to constant sum"
  } else if (rowNorm == "MedianNorm") {
    data <- t(apply(data, 1, MedianNorm))
    rownm <- "Normalization to sample median"
  } else if(rowNorm == "SpecNorm") {
    norm.vec <- rep(SpeWeight, nrow(data)) # default all same weight vec to prevent error
    data <- data / norm.vec
    message("No sample specific information were given, all set to 1.0")
    rownm <- "Normalization by sample-specific factor"
  } else {
    # nothing to do
    rownm <- "N/A"
  }
  ################################################ 
  
  # use apply will lose dimension info (i.e. row names and colnames)
  rownames(data) <- rowNames
  colnames(data) <- colNames
  
  # if the reference by feature, the feature column should be removed, since it is all 1
  if(rowNorm == "CompNorm" && !is.null(ref)){
    inx <- match(ref, colnames(data))
    data <- data[, -inx, drop=FALSE]
    colNames <- colNames[-inx]
  }
  
  #############################################
  #  Data transformation
  # generalize log, tolerant to 0 and negative values
  LogNorm <- function(x, min.val) {
    return(log10((x + sqrt(x^2 + min.val^2))/2))
  }
  
  # square root, tolerant to negative values
  SquareRootNorm <- function(x, min.val) {
    return(((x + sqrt(x^2 + min.val^2))/2)^(1/2))
  }
  
  if (transNorm == "LogNorm") {
    min.val <- min(abs(data[data != 0]))/10
    data <- apply(data, 2, LogNorm, min.val)
    transnm <- "Log10 Normalization"
  } else if (transNorm == "SrNorm") {
    min.val <- min(abs(data[data != 0]))/10
    data <- apply(data, 2, SquareRootNorm, min.val)
    transnm <- "Square Root Transformation"
  } else if (transNorm == "CrNorm") {
    norm.data <- abs(data)^(1/3)
    norm.data[data < 0] <- -norm.data[data < 0]
    data <- norm.data
    transnm <- "Cubic Root Transformation"
  } else {
    transnm <- "N/A"
  }
  #############################################
  
  #############################################
  # Data scaling
  # normalize to zero mean and unit variance
  AutoNorm <- function(x) {
    return((x - mean(x))/sd(x, na.rm = T))
  }
  
  # normalize to zero mean but variance/SE
  ParetoNorm <- function(x) {
    return((x - mean(x))/sqrt(sd(x, na.rm = T)))
  }
  
  # normalize to zero mean but variance/SE
  MeanCenter <- function(x) {
    return(x - mean(x))
  }
  
  # normalize to zero mean but variance/SE
  RangeNorm <- function(x) {
    if (max(x) == min(x)) {
      return(x)
    } else {
      return((x - mean(x))/(max(x) - min(x)))
    }
  }

  if (scaleNorm == "MeanCenter") {
    data <- apply(data, 2, MeanCenter)
    scalenm <- "Mean Centering"
  } else if (scaleNorm == "AutoNorm") {
    data <- apply(data, 2, AutoNorm)
    scalenm <- "Autoscaling"
  } else if (scaleNorm == "ParetoNorm") {
    data <- apply(data, 2, ParetoNorm)
    scalenm <- "Pareto Scaling"
  } else if (scaleNorm == "RangeNorm") {
    data <- apply(data, 2, RangeNorm)
    scalenm <- "Range Scaling"
  } else {
    scalenm <- "N/A"
  }
  ############################################# 
  
  message("Row norm: ", rownm, "\n",
          "Data Transformation norm: ", transnm, "\n",
          "Data Scaling norm: ", scalenm, "\n")  
  
  # note after using "apply" function, all the attribute lost, need to add back
  rownames(data) <- rowNames
  colnames(data) <- colNames
  
  target <- metadata_tab %>% 
    data.frame() %>%
    tibble::rownames_to_column("SampleID") %>%
    dplyr::filter(SampleID%in%rownames(data))
  se <- PomaSummarizedExperiment(target = target, 
                                 features = data)
  
  # need to do some sanity check, for log there may be Inf values introduced
  res <- CheckData(se)
  
  return(res)
}

se_normalize <- NormalizeData(
                      object = se_impute,
                      rowNorm = "None",
                      transNorm = "LogNorm",
                      scaleNorm = "ParetoNorm")

se_normalize
```

#### Normalization by **POMA** R package
```{r}
se_normalize_v2 <- PomaNorm(se_impute, method = "log_pareto")
se_normalize_v2
```

#### Comparison of unnormalized and normalized dataset

* boxplot
```{r, fig.width=14, fig.height=12}
pl_unnor <- PomaBoxplots(se_impute, group = "samples", jitter = FALSE) +
  ggtitle("Not Normalized") +
  theme(legend.position = "none") # data before normalization

pl_nor <- PomaBoxplots(se_normalize, group = "samples", jitter = FALSE) +
  ggtitle("Normalized") # data after normalization

cowplot::plot_grid(pl_unnor, pl_nor, ncol = 1, align = "v")
```

* density
```{r, fig.width=10, fig.height=8}
pl_unnor <- PomaDensity(se_impute, group = "features") +
  ggtitle("Not Normalized") +
  theme(legend.position = "none") # data before normalization

pl_nor <- PomaDensity(se_normalize, group = "features") +
  ggtitle("Normalized") # data after normalization

cowplot::plot_grid(pl_unnor, pl_nor, ncol = 1, align = "v")
```

## Cluster Analysis

### Hierarchical Clustering
```{r}
HieraCluster <- function(object,
                         method_dis = c("euclidean", "bray"),
                         method_cluster = c("average", "single", "complete", "ward", "ward.D2"),
                         cluster_type = c("Agglomerative", "Divisive"),
                         tree_num = 4) {
  
  features_tab <- SummarizedExperiment::assay(object) 
  metadata_tab <- SummarizedExperiment::colData(object)
  
  df <- t(features_tab)
  if (cluster_type == "Agglomerative") {
    # Agglomerative Hierarchical Clustering 
    # Dissimilarity matrix
    d <- dist(df, method = method_dis)
    
    # Hierarchical clustering using Linkage method
    hc <- hclust(d, method = method_cluster)
    # hc <- agnes(df, method = method_cluster)
    
    ####### identifying the strongest clustering structure ################
    # # methods to assess
    # m <- c( "average", "single", "complete", "ward")
    # names(m) <- c( "average", "single", "complete", "ward")
    # 
    # # function to compute coefficient
    # ac <- function(x) {
    #   agnes(df, method = x)$ac
    # }
    # 
    # map_dbl(m, ac)     
  } else if (cluster_type == "Divisive") {
    # Divisive Hierarchical Clustering 
    hc <- diana(df, metric = method_dis)
  }
  
  hc_res <- as.hclust(hc)
  sub_grp <- cutree(hc_res, k = tree_num)

  plot(hc_res, cex = 0.6)
  rect.hclust(hc_res, k = tree_num, border = 2:(tree_num+1)) 
  
  res <- list(data=df,
              cluster=sub_grp,
              hc=hc_res)
  
  return(res)
}
```

* Calculation
```{r, fig.width=8, fig.height=6}
Agg_hc_res <- HieraCluster(
      object = se_normalize,
      method_dis = "euclidean",
      method_cluster = "ward.D2",
      cluster_type = "Agglomerative",
      tree_num = 3)
```

* Visualization: visualize the result in a scatter plot
```{r, fig.width=8, fig.height=6}
fviz_cluster(list(data = Agg_hc_res$data, 
                  cluster = Agg_hc_res$cluster))
```


## Chemometrics Analysis

### Partial Least Squares-Discriminant Analysis (PLS-DA)

* Calculation

```R
poma_plsda <- PomaMultivariate(se_normalize, method = "plsda")
```

```{r, include=FALSE}
poma_plsda <- PomaMultivariate(se_normalize, method = "plsda")
```

* scatter plot
```{r, fig.width=8, fig.height=6}
poma_plsda$scoresplot +
  ggtitle("Scores Plot (plsda)")
```

* errors plot
```{r, fig.width=8, fig.height=5}
poma_plsda$errors_plsda_plot +
  ggtitle("Error Plot (plsda)")
```

### Sparse Partial Least Squares-Discriminant Analysis (sPLS-DA)

> Even though PLS is highly efficient in a high dimensional context, the interpretability of PLS needed to be improved. sPLS has been recently developed by our team to perform simultaneous variable selection in both data sets X and Y data sets, by including LASSO penalizations in PLS on each pair of loading vectors

* Calculation

```R
poma_splsda <- PomaMultivariate(se_normalize, method = "splsda")
```

```{r, include=FALSE}
poma_splsda <- PomaMultivariate(se_normalize, method = "splsda")
```

* scatter plot
```{r, fig.width=8, fig.height=6}
poma_splsda$scoresplot +
  ggtitle("Scores Plot (splsda)")
```

## Univariate Analysis

### Fold Change Analysis
```{r}
FoldChange <- function(object,
                       group_names,
                       fc_cutoff = 2,
                       cmp_type = 0, 
                       paired = FALSE) {
  
  features_tab <- SummarizedExperiment::assay(object) 
  metadata_tab <- SummarizedExperiment::colData(object)   
  
  metadata <- metadata_tab %>%
    data.frame() %>%
    dplyr::filter(group %in% group_names)
  profile <- features_tab[, pmatch(rownames(metadata), colnames(features_tab))]
  
  # make sure threshold is above 1
  fc_cutoff <- ifelse(fc_cutoff > 1, fc_cutoff, 1/fc_cutoff)
  max_cutoff <- fc_cutoff
  min_cutoff <- 1 / fc_cutoff
  
  GetFC <- function(x,
                    y,
                    cmpType, 
                    paired = FALSE) {

    # x = profile
    # y = metadata
    # cmpType = cmp_type
    # paired = paired    
    
    x <- t(x)
    colNames <- colnames(x)
    rowNames <- rownames(x)
    
    if (paired) { 
      # compute the average of paired FC (unit is pair)
      G1 <- x[which(y$group == group_names[1]), ]
      G2 <- x[which(y$group == group_names[2]), ]
      
      if (cmpType == 0) {
        fc.mat <- G1 - G2
      } else {
        fc.mat <- G2 - G1
      }
      fc.log <- apply(fc.mat, 2, mean)
      fc.all <- signif(2^fc.log, 5)
    } else {
      m1 <- colMeans(x[which(y$group == group_names[1]), ])
      m2 <- colMeans(x[which(y$group == group_names[2]), ])
        
      # create a named matrix of sig vars for display
      if (cmpType == 0) {
          ratio <- m1/m2
      } else {
          ratio <- m2/m1
      }
      
      fc.all <- signif(ratio, 5)
      ratio[ratio < 0] <- 0
      fc.log <- signif(log2(ratio), 5)
      fc.log[is.infinite(fc.log) & fc.log < 0] <- -99
      fc.log[is.infinite(fc.log) & fc.log > 0] <- 99
    }
    
    names(fc.all) <- names(fc.log) <- colnames(x)
    
    res <- list(fc.all = fc.all, fc.log = fc.log)
    return(res)
  }  
  
  res <- GetFC(profile, metadata, cmp_type, paired)
  fc.all <- res$fc.all
  fc.log <- res$fc.log
  
  inx.up <- fc.all > max_cutoff
  inx.down <- fc.all < min_cutoff
  names(inx.up) <- names(inx.down) <- names(fc.all)
  imp.inx <- inx.up | inx.down
  sig.mat <- cbind(fc.all[imp.inx, drop = F], fc.log[imp.inx, drop = F])
  colnames(sig.mat) <- c("Fold Change", "log2(FC)")
  
  # order by absolute log value (since symmetrical in pos and neg)
  inx.ord <- order(abs(sig.mat[, 2]), decreasing = T)
  sig.mat <- sig.mat[inx.ord, , drop = F]
  
  return(sig.mat)
}

FC_res <- FoldChange(
           object = se_normalize, 
           fc_cutoff = 2,
           group_names = c("NR", "PR"),
           cmp_type = 0, 
           paired = FALSE)

head(FC_res)
```


### T Test
```{r}
group_names <- c("NR", "PR")

se_normalize_subset <- se_normalize[, se_normalize$group %in% group_names]
se_normalize_subset$group <- factor(as.character(se_normalize_subset$group))
ttest_res <- PomaUnivariate(se_normalize_subset, method = "ttest")
head(ttest_res)
```


### Volcano plot
```{r, warning=FALSE, fig.height=5, fig.width=6}
se_impute_subset <- se_impute[, se_impute$group %in% group_names]
se_impute_subset$group <- factor(as.character(se_impute_subset$group))

PomaVolcano(se_impute_subset, 
            pval = "raw",
            pval_cutoff = 0.05,
            log2FC = 0.5,
            xlim = 3,
            labels = TRUE,
            plot_title = TRUE)
```

## Feature Selection

### Regularized Generalized Linear Models (Lasso: alpha = 1)
```{r, fig.width=12, fig.height=6}
lasso_res <- PomaLasso(se_normalize_subset, alpha = 1, labels = TRUE)

cowplot::plot_grid(lasso_res$cvLassoPlot,
                   lasso_res$coefficientPlot,
                   ncol = 2, align = "h")

lasso_res$coefficients
```


### Classification (Random Forest)

* Calculation
```{r, fig.width=6, fig.height=4}
poma_rf <- PomaRandForest(se_normalize_subset, ntest = 10, nvar = 10)
poma_rf$error_tree
```

* table
```{r}
poma_rf$confusionMatrix$table
```

* Important features
```{r, fig.width=6, fig.height=4}
poma_rf$MeanDecreaseGini_plot
```


## Network Analysis

### Data curation
```{r}
features_tab <- SummarizedExperiment::assay(se_filter) %>% 
  t()
features_tab[is.na(features_tab)] <- 0

print(features_tab[1:6, 1:10])
```


### Building network model
```{r}
net_single <- netConstruct(features_tab, 
                           measure = "sparcc",
                           measurePar = list(iter = 20,
                                             inner_iter = 10,
                                             th = 0.1),
                           filtTax = "highestVar",
                           filtTaxPar = list(highestVar = 50),
                           filtSamp = "totalReads",
                           filtSampPar = list(totalReads = 100),
                           verbose = 3,
                           seed = 123)
```

### Visualizing the network
```{r, fig.height=8, fig.width=12}
props_single <- netAnalyze(net_single, clustMethod = "cluster_fast_greedy")

plot(props_single, 
     nodeColor = "cluster", 
     nodeSize = "eigenvector",
     repulsion = 0.8,
     rmSingles = TRUE,
     labelScale = FALSE,
     cexLabels = 1.6,
     nodeSizeSpread = 3,
     cexNodes = 2,
     title1 = "Network on metabolomics with Pearson correlations", 
     showTitle = TRUE,
     cexTitle = 1.5)

legend(0.7, 1.1, cex = 1.2, title = "estimated correlation:",
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"),
       bty = "n", horiz = TRUE)
```


## Network Analysis by WGCNA 

Performing Network Analysis step by step through **WGCNA** R package.

### Data curation

* Data Matrix

    * Row -> metabolites

    * Column -> samples

```{r}
features_tab <- SummarizedExperiment::assay(se_impute)
print(features_tab[1:6, 1:10])
```

* Data normalization
```{r}
# TSS
features_tab_norm <- XMAS2::norm_tss(phyloseq::otu_table(features_tab, taxa_are_rows = T)) %>% 
  data.frame() %>% t()
print(features_tab_norm[1:6, 1:10])

# # CSS
# features_tab_norm <- XMAS2::norm_css(phyloseq::otu_table(features_tab, taxa_are_rows = T)) %>% 
#   data.frame() %>% t()
# print(features_tab_norm[1:6, 1:10])
```


### Tuning soft thresholds

Picking a threshhold value (if correlation is below threshold, remove the edge). WGCNA will try a range of soft thresholds and create a diagnostic plot.

* Choose a set of soft-thresholding powers
```{r}
powers <- c(c(1:10), seq(12, 20, 2))
```


* Call the network topology analysis function

    * Row -> samples

    * Column -> metabolites

```{r}
sft <- pickSoftThreshold(
  features_tab_norm,
  powerVector = powers,
  networkType = "unsigned",
  verbose = 2)
```

* the optimal power value
```{r, fig.width=8, fig.height=6}
par(mfrow = c(1, 2))
cex1 = 1.2

plot(sft$fitIndices[, 1],
     -sign(sft$fitIndices[, 3]) * sft$fitIndices[, 2],
     xlab = "Soft Threshold (power)",
     ylab = "Scale Free Topology Model Fit, signed R^2",
     main = paste("Scale independence")
)
text(sft$fitIndices[, 1],
     -sign(sft$fitIndices[, 3]) * sft$fitIndices[, 2],
     labels = powers, cex = cex1, col = "red"
)
abline(h = 0.90, col = "red")
plot(sft$fitIndices[, 1],
     sft$fitIndices[, 5],
     xlab = "Soft Threshold (power)",
     ylab = "Mean Connectivity",
     type = "n",
     main = paste("Mean connectivity")
)
text(sft$fitIndices[, 1],
     sft$fitIndices[, 5],
     labels = powers,
     cex = cex1, col = "red")
```

**Notice**: We’ pick **5** but feel free to experiment with other powers to see how it affects your results.


### Create the network using the `blockwiseModules`

* building network
```{r}
picked_power <- 5
# temp_cor <- cor       
# cor <- WGCNA::cor 
netwk <- blockwiseModules(features_tab_norm,
                          
                          # == Adjacency Function ==
                          power = picked_power,                # <= power here
                          networkType = "signed",
                          
                          # == Network construction arguments: correlation options
                          corType = "bicor",
                          maxPOutliers = 0.05,

                          # == Tree and Block Options ==
                          deepSplit = 2,
                          pamRespectsDendro = F,
                          minModuleSize = 20,
                          maxBlockSize = 4000,

                          # == Module Adjustments ==
                          reassignThreshold = 0,
                          mergeCutHeight = 0.25,

                          # == TOM == Archive the run results in TOM file (saves time)
                          saveTOMs = T,
                          saveTOMFileBase = paste0("./dataset/", "GvHD"),

                          # == Output Options
                          numericLabels = T,
                          verbose = 3,
                          randomSeed = 123)
```

* Modules' number 
```{r}
table(netwk$colors)
```


* hubs
```{r}
rownames(netwk$MEs) <- rownames(features_tab_norm)
names(netwk$colors) <- colnames(features_tab_norm)
names(netwk$unmergedColors) <- colnames(features_tab_norm)

hubs <- chooseTopHubInEachModule(features_tab_norm, netwk$colors)

hubs
```

* The number of features per module
```{r, fig.width=8, fig.height=6}
table(netwk$colors) %>% 
  data.frame() %>% 
  dplyr::rename(Module = Var1, Number = Freq) %>% 
  dplyr::mutate(Module_color = labels2colors(as.numeric(as.character(Module)))) %>% 
  ggplot(aes(x = Module, y = Number, fill = Module)) +
      geom_text(aes(label = Number), vjust = 0.5, hjust = -0.18, size = 3.5) +
      geom_col(color = "#000000") +
      ggtitle("Number of features per module") +
      coord_flip() +
      scale_y_continuous(expand = c(0, 0)) +
      theme_classic() +
      theme(plot.margin = margin(2, 2, 2, 2, "pt"),
            plot.title = element_text(size = 14, hjust = 0.5, face = "bold"),
            legend.position = "none")
```


### plot modules
```{r, fig.width=8, fig.height=6}
mergedColors <- labels2colors(netwk$colors)

plotDendroAndColors(
  netwk$dendrograms[[1]],
  mergedColors[netwk$blockGenes[[1]]],
  "Module colors",
  dendroLabels = FALSE,
  hang = 0.03,
  addGuide = TRUE,
  guideHang = 0.05 )
```

### Relationships among modules
```{r, fig.width=8, fig.height=6}
plotEigengeneNetworks(netwk$MEs, 
                      "Eigengene adjacency heatmap",
                      marDendro = c(3, 3, 2, 4),
                      marHeatmap = c(3, 4, 2, 2), 
                      plotDendrograms = T,
                      xLabelsAngle = 90)
```


### Module (Eigengene) correlation
```{r, fig.width=6, fig.height=4}
MEs <- netwk$MEs
MEs_R <- bicor(MEs, MEs, maxPOutliers = 0.05)

idx.r <- which(rownames(MEs_R) == "ME0")
idx.c <- which(colnames(MEs_R) == "ME0")

MEs_R_noME0 <- MEs_R[-idx.r, -idx.c]

MEs_R_density <- MEs_R[upper.tri(MEs_R_noME0)] %>% 
  as.data.frame() %>% 
  dplyr::rename("correlation" = ".") %>% 
  ggplot(aes(x=correlation)) + 
  geom_density() + 
  ggtitle(paste0("ME correlation density\n without ", "ME0"))

MEs_R_Corr <- pheatmap::pheatmap(MEs_R, color = colorRampPalette(c("Blue", "White", "Red"))(100),
                   silent = T, 
                   breaks = seq(-1,1,length.out = 101),
                   treeheight_row = 5, 
                   treeheight_col = 5,
                   main = paste0("ME correlation heatmap"),
                   labels_row = rownames(MEs_R),
                   labels_col =  colnames(MEs_R)) 

cowplot::plot_grid(MEs_R_density, MEs_R_Corr$gtable, 
                   labels = c("A", "B"), 
                   label_size = 15, 
                   rel_widths = c(0.6, 1),
                   align = "h") 
```


### Relate Module (cluster) Assignments to Groups
```{r, fig.width=8, fig.height=6}
module_df <- data.frame(
  featureID = names(netwk$colors),
  colors = labels2colors(netwk$colors)
)

# Get Module Eigengenes per cluster
MEs0 <- moduleEigengenes(features_tab_norm, mergedColors)$eigengenes

# Reorder modules so similar modules are next to each other
MEs0 <- orderMEs(MEs0)
module_order <- names(MEs0) %>% gsub("ME","", .)

# Add group names
MEs0$group <- paste0(se_impute$group, rownames(colData(se_impute)))  # row.names(MEs0) == rownames(colData(se_impute))

# tidy & plot data
mME <- MEs0 %>%
  tidyr::pivot_longer(-group) %>%
  mutate(
    name = gsub("ME", "", name),
    name = factor(name, levels = module_order)
  )

mME %>% ggplot(., aes(x=group, y=name, fill=value)) +
  geom_tile() +
  labs(title = "Module-samples Relationships", y = "Modules", fill = "corr") +  
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0,
    limit = c(-1,1)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90))
```

**Result**: 

1. the *black* modules seems negatively associated (red shading) with the PR groups.


### Generate and Export Networks
```{r}
# modules_of_interest <- c("green", "brown", "black")
# 
# genes_of_interest <- module_df %>%
#   subset(colors %in% modules_of_interest)
# 
# expr_of_interest <- features_tab_norm[, genes_of_interest$featureID]
# 
# # Only recalculate TOM for modules of interest 
# TOM <- TOMsimilarityFromExpr(expr_of_interest,
#                              power = picked_power)
# 
# # Add feature id to row and columns
# rownames(TOM) <- colnames(expr_of_interest)
# colnames(TOM) <- colnames(expr_of_interest)
# 
# edge_list <- data.frame(TOM) %>%
#   tibble::rownames_to_column("featureID") %>%
#   tidyr::pivot_longer(-featureID) %>%
#   dplyr::rename(featureID2 = name, correlation = value) %>%
#   unique() %>%
#   subset(!(featureID == featureID2)) %>%
#   dplyr::mutate(
#     module1 = module_df[featureID, ]$colors,
#     module2 = module_df[featureID2, ]$colors)
# 
# head(edge_list)
```


### Network visualization
```{r, fig.width=8, fig.height=6}
library(igraph)

strength_adjust <- 1
TOM <- TOMsimilarityFromExpr(features_tab_norm,
                             power = picked_power)
# Add feature id to row and columns
rownames(TOM) <- colnames(features_tab_norm)
colnames(TOM) <- colnames(features_tab_norm)

g <- graph.adjacency(TOM, mode="undirected", weighted= TRUE)
delete.edges(g, which(E(g)$weight <1))
E(g)$width <- E(g)$weight*strength_adjust + min(E(g)$weight)
E(g)$color <- "red"

plot(g)
```


## Systematic Information
```{r}
devtools::session_info()
```