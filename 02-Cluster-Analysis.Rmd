# Cluster Analysis

> Hierarchical clustering is an alternative approach to k-means clustering for identifying groups in the dataset. It does not require us to pre-specify the number of clusters to be generated as is required by the k-means approach. Furthermore, hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram.


## Loading packages
```{r}
knitr::opts_chunk$set(warning = F)

library(dplyr)
library(tibble)
library(POMA)
library(ggplot2)
library(SummarizedExperiment)
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms

# rm(list = ls())
options(stringsAsFactors = F)
options(future.globals.maxSize = 1000 * 1024^2)
```

## Importing data

The input data sets are from the previous chapter.
```{r}
se_normalize <- readRDS("./dataset/POMA/se_normalize.RDS")
```


## Hierarchical Clustering

Hierarchical clustering can be divided into two main types: **agglomerative** and **divisive**.


Calculate dissimilarity

> However, a bigger question is: How do we measure the dissimilarity between two clusters of observations? A number of different cluster agglomeration methods (i.e, linkage methods) have been developed to answer to this question. The most common types methods are:
>
> 1. **Maximum or complete linkage clustering**: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the largest value (i.e., maximum value) of these dissimilarities as the distance between the two clusters. It tends to produce more compact clusters.
2. **Minimum or single linkage clustering**: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion. It tends to produce long, “loose” clusters.
3. **Mean or average linkage clustering**: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the average of these dissimilarities as the distance between the two clusters.
4. **Centroid linkage clustering**: It computes the dissimilarity between the centroid for cluster 1 (a mean vector of length p variables) and the centroid for cluster 2.
5. **Ward’s minimum variance method**: It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged.

Data Processing:

1. Rows are observations (individuals) and columns are variables.

2. Any missing value in the data must be removed or estimated.

3. The data must be standardized (i.e., scaled) to make variables comparable. Recall that, standardization consists of transforming the variables such that they have mean zero and standard deviation one.


Functions to computing hierarchical clustering:

1. *hclust* [in stats package] and *agnes* [in cluster package] for agglomerative hierarchical clustering.

2. *diana* [in cluster package] for divisive hierarchical clustering.

```{r}
HieraCluster <- function(object,
                         method_dis = c("euclidean", "bray"),
                         method_cluster = c("average", "single", "complete", "ward", "ward.D2"),
                         cluster_type = c("Agglomerative", "Divisive"),
                         tree_num = 4) {
  
  features_tab <- SummarizedExperiment::assay(object) 
  metadata_tab <- SummarizedExperiment::colData(object)
  
  df <- t(features_tab)
  if (cluster_type == "Agglomerative") {
    # Agglomerative Hierarchical Clustering 
    # Dissimilarity matrix
    d <- dist(df, method = method_dis)
    
    # Hierarchical clustering using Linkage method
    hc <- hclust(d, method = method_cluster)
    # hc <- agnes(df, method = method_cluster)
    
    ####### identifying the strongest clustering structure ################
    # # methods to assess
    # m <- c( "average", "single", "complete", "ward")
    # names(m) <- c( "average", "single", "complete", "ward")
    # 
    # # function to compute coefficient
    # ac <- function(x) {
    #   agnes(df, method = x)$ac
    # }
    # 
    # map_dbl(m, ac)     
  } else if (cluster_type == "Divisive") {
    # Divisive Hierarchical Clustering 
    hc <- diana(df, metric = method_dis)
  }
  
  hc_res <- as.hclust(hc)
  sub_grp <- cutree(hc_res, k = tree_num)

  plot(hc_res, cex = 0.6)
  rect.hclust(hc_res, k = tree_num, border = 2:(tree_num+1)) 
  
  res <- list(data=df,
              cluster=sub_grp,
              hc=hc_res)
  
  return(res)
}
```


### Agglomerative Hierarchical Clustering

> **Agglomerative clustering**: It’s also known as AGNES (Agglomerative Nesting). It works in a bottom-up manner. That is, each object is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are member of just one single big cluster (root). The result is a tree which can be plotted as a dendrogram.

* Calculation
```{r, fig.width=8, fig.height=6}
Agg_hc_res <- HieraCluster(
      object = se_normalize,
      method_dis = "euclidean",
      method_cluster = "ward.D2",
      cluster_type = "Agglomerative")
```

* Visualization: visualize the result in a scatter plot
```{r, fig.width=8, fig.height=6}
fviz_cluster(list(data = Agg_hc_res$data, 
                  cluster = Agg_hc_res$cluster))
```


### Divisive Hierarchical Clustering

> **Divisive hierarchical clustering**: It’s also known as DIANA (Divise Analysis) and it works in a top-down manner. The algorithm is an inverse order of AGNES. It begins with the root, in which all objects are included in a single cluster. At each step of iteration, the most heterogeneous cluster is divided into two. The process is iterated until all objects are in their own cluster.

* Calculation
```{r, fig.width=8, fig.height=6}
Div_hc_res <- HieraCluster(
      object = se_normalize,
      method_dis = "euclidean",
      method_cluster = "ward",
      cluster_type = "Divisive")
```

* Visualization: visualize the result in a scatter plot
```{r, fig.width=8, fig.height=6}
fviz_cluster(list(data = Div_hc_res$data, 
                  cluster = Div_hc_res$cluster))
```


### Comparison

* dendrograms: In the dendrogram displayed above, each leaf corresponds to one observation.
```{r, fig.width=8, fig.height=6}
Agg_hc_dend <- as.dendrogram(Agg_hc_res$hc)
Div_hc_dend <- as.dendrogram(Div_hc_res$hc)

tanglegram(Agg_hc_dend, Div_hc_dend)
```

* tanglegrams
```{r, fig.width=8, fig.height=6}
dend_list <- dendlist(Agg_hc_dend, Div_hc_dend)
tanglegram(Agg_hc_dend, Div_hc_dend,
  highlight_distinct_edges = FALSE, # Turn-off dashed lines
  common_subtrees_color_lines = FALSE, # Turn-off line colors
  common_subtrees_color_branches = TRUE, # Color common branches 
  main = paste("entanglement =", round(entanglement(dend_list), 2)))
```


### Determining Optimal Clusters

* Elbow plot
```{r, fig.width=6, fig.height=4}
fviz_nbclust(Agg_hc_res$data, FUN = hcut, method = "wss")
```

* Average Silhouette Method
```{r, fig.width=6, fig.height=4}
fviz_nbclust(Agg_hc_res$data, FUN = hcut, method = "silhouette")
```

* Gap Statistic Method
```{r, fig.width=6, fig.height=4}
gap_stat <- clusGap(Agg_hc_res$data, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```


## Partitional Clustering

> K-means clustering is the most commonly used unsupervised machine learning algorithm for partitioning a given data set into a set of k groups (i.e. k clusters), where k represents the number of groups pre-specified by the analyst.

```{r, fig.width=8, fig.height=6}
PartCluster <- function(object,
                        cluster_num = 4) {
  
  features_tab <- SummarizedExperiment::assay(object) 
  metadata_tab <- SummarizedExperiment::colData(object)
  df <- t(features_tab)
  
  res <- kmeans(df, centers = cluster_num)  
  # show clusters
  print(fviz_cluster(list(data = df, 
                    cluster = res$cluster))) 
  
  return(res)
}

Kcluster_res <- PartCluster(
        object = se_normalize,
        cluster_num = 4)
```


## Systematic Information
```{r}
devtools::session_info()
```
