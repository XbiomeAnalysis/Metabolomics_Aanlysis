# Data Processing

Although the horrible experience of data analysis by using **MetaboAnalystR** R package [@pang2020metaboanalystr], its thought of data processing are very useful. Therefore, this template is based on the workflow from **MetaboAnalystR**.

We integrated R packages and our own scripts to build the data analysis template on metabolomic data. Particularly, we thanks very much for **POMA** R package [@castellano2021pomashiny]. **POMA** is a flexible data cleaning and statistical analysis processes in one comprehensible and user-friendly R package.

## Loading packages
```{r}
knitr::opts_chunk$set(warning = F)

library(dplyr)
library(tibble)
library(POMA)
library(ggplot2)
library(ggraph)
library(plotly)
library(readxl)
library(SummarizedExperiment)

# rm(list = ls())
options(stringsAsFactors = F)
options(future.globals.maxSize = 1000 * 1024^2)
```


## Importing data 

The dataset is from the Zeybel-2022 published paper [@zeybel2022multiomics].

* features table
```{r}
profile <- readxl::read_xlsx("./dataset/OmicsDataSet-Zeybel-2022.xlsx", sheet = 6)
head(profile)
```

* metadata table
```{r}
metadata <- readxl::read_xlsx("./dataset/OmicsDataSet-Zeybel-2022.xlsx", sheet = 2)
head(metadata)
```

* Data Preparation: SummarizedExperiment object
```{r}
getSEobject <- function(x, y) {

  target <- x %>% 
    dplyr::mutate(Metabolomics == "Send") %>%
    dplyr::select(PatientID, LiverFatClass, Gender, Smoker, Age, AlcoholConsumption)

  sid <- intersect(target$PatientID, colnames(profile))
  
  features <- y %>% 
    dplyr::select(all_of(sid)) %>%
    data.frame() %>% t()
  colnames(features) <- paste0("M_", profile$`COMP ID`)
  
  target <- target[pmatch(sid, target$PatientID), , F]
  
  res <- PomaSummarizedExperiment(target = target, 
                                  features = features)
  return(res)  
}

se <- getSEobject(metadata, profile)

se
```

* Extract data for test dataset
```{r}
get_testData <- function(object, num = 200) {

  features_tab <- SummarizedExperiment::assay(object) %>%
    t()
  metadata_tab <- SummarizedExperiment::colData(object) %>%
    data.frame() %>%
    tibble::rownames_to_column("ID")
  
  res <- PomaSummarizedExperiment(target = metadata_tab, 
                                  features = features_tab[, 1:num])
  return(res)
}

se <- get_testData(object = se)
se
```


## Data Checking

Features in PomaSummarizedExperiment object must have the following criterion:

* All data values are numeric.

* A total of 0 (0%) missing values were detected.
```{r}
CheckData <- function(object) {
  
  features_tab <- SummarizedExperiment::assay(object)
  
  # numeric & missing values
  int_mat <- features_tab
  rowNms <- rownames(int_mat)
  colNms <- colnames(int_mat)
  naNms <- sum(is.na(int_mat))
  for (i in 1:ncol(int_mat)) {
    if (class(int_mat[, i]) == "integer64") {
      int_mat[, i] <- as.double(int_mat[, i])
    }
  }
  
  num_mat <- apply(int_mat, 2, as.numeric)
  if (sum(is.na(num_mat)) > naNms) {
    num_mat <- apply(int_mat, 2, function(x) as.numeric(gsub(",",  "", x)))
    if (sum(is.na(num_mat)) > naNms) {
      message("<font color=\"red\">Non-numeric values were found and replaced by NA.</font>")
    } else {
      message("All data values are numeric.")
    }
  } else {
    message("All data values are numeric.")
  }
  
  int_mat <- num_mat
  rownames(int_mat) <- rowNms
  colnames(int_mat) <- colNms
  varCol <- apply(int_mat, 2, var, na.rm = T)
  constCol <- (varCol == 0 | is.na(varCol))
  constNum <- sum(constCol, na.rm = T)
  if (constNum > 0) {
    message(paste("<font color=\"red\">", constNum, 
      "features with a constant or single value across samples were found and deleted.</font>"))
    int_mat <- int_mat[, !constCol, drop = FALSE]
  }
  
  totalCount <- nrow(int_mat) * ncol(int_mat)
  naCount <- sum(is.na(int_mat))
  naPercent <- round(100 * naCount/totalCount, 1)

  message(paste("A total of ", naCount, " (", naPercent, 
    "%) missing values were detected.", sep = ""))  
  
  # save int_mat into se object 
  target <- SummarizedExperiment::colData(object) %>%
    data.frame() %>%
    tibble::rownames_to_column("SampleID")
  res <- PomaSummarizedExperiment(target = target, 
                                  features = t(int_mat))
  
  return(res)
}

se <- CheckData(object = se)
se
```


## Data Filtering

The purpose of the data filtering is to identify and remove variables that are unlikely to be of use when modeling the data. No phenotype information are used in the filtering process, so the result can be used with any downstream analysis. This step is strongly recommended for untargeted metabolomics datasets (i.e. spectral binning data, peak lists) with large number of variables, many of them are from baseline noises. Filtering can usually improve the results. For details, please refer to the paper by Hackstadt, et al.

Non-informative variables can be characterized in three groups: 1) variables of very small values (close to baseline or detection limit) - these variables can be detected using mean or median; 2) variables that are near-constant values throughout the experiment conditions (housekeeping or homeostasis) - these variables can be detected using standard deviation (SD); or the robust estimate such as interquantile range (IQR); and 3) variables that show low repeatability - this can be measured using QC samples using the relative standard deviation(RSD = SD/mean). Features with high percent RSD should be removed from the subsequent analysis (the suggested threshold is 20% for LC-MS and 30% for GC-MS). For data filtering based on the first two categories, the following empirical rules are applied during data filtering:

* Less than 250 variables: 5% will be filtered;
* Between 250 - 500 variables: 10% will be filtered;
* Between 500 - 1000 variables: 25% will be filtered;
* Over 1000 variables: 40% will be filtered;


Filtering features if their RSDs are > 25% in QC samples

* Interquantile range (IQR)

* Standard deviation (SD)

* Median absolute deviation (MAD)

* Relative standard deviation (RSD = SD/mean)

* Non-parametric relative standard deviation (MAD/median)

* Mean intensity value

* Median intensity value

```{r}
FilterFeature <- function(
                       object,
                       qc_label,
                       method = c("none", "iqr", "rsd", 
                                  "nrsd", "mean", "sd",
                                  "mad", "median"),
                       rsd_cutoff = 25) {
  
  features_tab <- SummarizedExperiment::assay(object) 
  metadata_tab <- SummarizedExperiment::colData(object) 
  
  # QC samples
  qc_samples <- metadata_tab %>% data.frame() %>%
    dplyr::filter(group == qc_label)
  if (dim(qc_samples)[1] == 0) {
    stop("No qc samples have been chosen, please check your input")
  }
  
  # QC samples' feature table
  qc_feature <- features_tab[, colnames(features_tab)%in%rownames(qc_samples)] %>%
    t()
  
  # filter features by QC RSD
  rsd <- rsd_cutoff / 100
  sds <- apply(qc_feature, 2, sd, na.rm = T)
  mns <- apply(qc_feature, 2, mean, na.rm = T)
  rsd_vals <- abs(sds/mns) %>% na.omit()
  gd_inx <- rsd_vals < rsd
  int_mat <- features_tab[gd_inx, ]
  message("Removed ", (dim(qc_feature)[2] - dim(int_mat)[1]), 
  " features based on QC RSD values. QC samples are excluded from downstream functional analysis.")
  
  # whether to filter features by percentage according to the number
  PerformFeatureFilter <- function(datMatrix, 
                                   qc_method = method,
                                   remain_num = NULL) {
    
    dat <- datMatrix
    feat_num <- ncol(dat)
    feat_nms <- colnames(dat)
    nm <- NULL
    if (qc_method == "none" && feat_num < 5000) { # only allow for less than 4000
      remain <- rep(TRUE, feat_num)
      nm <- "No filtering was applied"
    } else {
      if (qc_method == "rsd"){
        sds <- apply(dat, 2, sd, na.rm = T)
        mns <- apply(dat, 2, mean, na.rm = T)
        filter_val <- abs(sds/mns)
        nm <- "Relative standard deviation"
      } else if (qc_method == "nrsd" ) {
        mads <- apply(dat, 2, mad, na.rm = T)
        meds <- apply(dat, 2, median, na.rm = T)
        filter_val <- abs(mads/meds)
        nm <- "Non-paramatric relative standard deviation"
      } else if (qc_method == "mean") {
        filter_val <- apply(dat, 2, mean, na.rm = T)
        nm <- "mean"
      } else if (qc_method == "sd") {
        filter_val <- apply(dat, 2, sd, na.rm = T)
        nm <- "standard deviation"
      } else if (qc_method == "mad") {
        filter_val <- apply(dat, 2, mad, na.rm = T)
        nm <- "Median absolute deviation"
      } else if (qc_method == "median") {
        filter_val <- apply(dat, 2, median, na.rm = T)
        nm <- "median"
      } else if (qc_method == "iqr") { # iqr
        filter_val <- apply(dat, 2, IQR, na.rm = T)
        nm <- "Interquantile Range"
      }
      
      # get the rank of the filtered variables
      rk <- rank(-filter_val, ties.method = "random")
      
      if (is.null(remain_num)) { # apply empirical filtering based on data size
          if (feat_num < 250) { # reduce 5%
            remain <- rk < feat_num * 0.95
            message("Further feature filtering based on ", nm)
          } else if (feat_num < 500) { # reduce 10%
            remain <- rk < feat_num * 0.9
            message("Further feature filtering based on ", nm)
          } else if (feat_num < 1000) { # reduce 25%
            remain <- rk < feat_num * 0.75
            message("Further feature filtering based on ", nm)
          } else { # reduce 40%, if still over 5000, then only use top 5000
            remain <- rk < feat_num * 0.6
            message("Further feature filtering based on ", nm)
          }
      } else {
        remain <- rk < remain_num
      }
    }
    
    res <- datMatrix[, remain]
    
    return(res)
  }  
  
  feature_res <- PerformFeatureFilter(t(int_mat))
  
  # remove QC samples 
  feature_final <- feature_res[!rownames(feature_res) %in% rownames(qc_samples), ]
  
  # save int_mat into se object 
  target <- metadata_tab %>% 
    data.frame() %>%
    tibble::rownames_to_column("SampleID") %>%
    dplyr::filter(SampleID %in% rownames(feature_final))
  res <- PomaSummarizedExperiment(target = target, 
                                  features = feature_final)
  
  return(res) 
}

se_filter <- FilterFeature(object = se,
                           qc_label = "None",
                           method = "iqr")
se_filter
```

## Missing Value Imputation
```{r}
se_impute <- PomaImpute(
                    se_filter, 
                    ZerosAsNA = TRUE, 
                    RemoveNA = TRUE, 
                    cutoff = 20, 
                    method = "knn")
se_impute
```

## Data Normalization

The normalization procedures are grouped into three categories. You can use one or combine them to achieve better results.

* Sample normalization is for general-purpose adjustment for systematic differences among samples;

  * Sample-specific normalization (i.e. weight, volume)
  
  * Normalization by sum	
  
  * Normalization by median	
  
  * Normalization by a reference sample (PQN)	
  
  * Normalization by a pooled sample from group (group PQN)	
  
  * Normalization by reference feature
  
  * Quantile normalization (suggested only for > 1000 features)	
  

* Data transformation applies a mathematical transformation on individual values themselves. A simple mathematical approach is used to deal with negative values in log and square root.

  * Log transformation (base 10)
  
  * Square root transformation (square root of data values)
  
  * Cube root transformation (cube root of data values)


* Data scaling adjusts each variable/feature by a scaling factor computed based on the dispersion of the variable.

  * Mean centering	(mean-centered only)
  
  * Auto scaling	(mean-centered and divided by the standard deviation of each variable)
  
  * Pareto scaling	(mean-centered and divided by the square root of the standard deviation of each variable)
  
  * Range scaling	(mean-centered and divided by the range of each variable)

### Normalization by **NormalizeData** function
```{r}
NormalizeData <- function(
                      object,
                      rowNorm = c("Quantile", "GroupPQN", "SamplePQN", 
                                  "CompNorm", "SumNorm", "MedianNorm",
                                  "SpecNorm", "None"),
                      transNorm = c("LogNorm", "SrNorm", "CrNorm", "None"),
                      scaleNorm = c("MeanCenter", "AutoNorm", "ParetoNorm", 
                                    "RangeNorm", "None"),
                      ref = NULL,
                      SpeWeight = 1) {
  
  features_tab <- SummarizedExperiment::assay(object) 
  metadata_tab <- SummarizedExperiment::colData(object)   
  
  data <- t(features_tab)
  colNames <- colnames(data)
  rowNames <- rownames(data)
  
  #############################################
  # Sample normalization
  # perform quantile normalization on the raw data (can be log transformed later by user)
  QuantileNormalize <- function(data) {
    return(t(preprocessCore::normalize.quantiles(t(data), copy=FALSE)));
  }
  # normalize by a reference sample (probability quotient normalization)
  # ref should be the name of the reference sample
  ProbNorm <- function(x, ref_smpl) {
    return(x/median(as.numeric(x/ref_smpl), na.rm = T))
  }
  
  # normalize by a reference reference (i.e. creatinine)
  # ref should be the name of the cmpd
  CompNorm <- function(x, ref) {
    return(1000*x/x[ref])
  }
  
  SumNorm <- function(x) {
    return(1000*x/sum(x, na.rm = T))
  }
  
  # normalize by median
  MedianNorm <- function(x) {
    return(x/median(x, na.rm = T))
  }  
  
  # row-wise normalization
  if (rowNorm == "Quantile") {
    data <- QuantileNormalize(data)
    # this can introduce constant variables if a variable is 
    # at the same rank across all samples (replaced by its average across all)
    varCol <- apply(data, 2, var, na.rm = T)
    constCol <- (varCol == 0 | is.na(varCol))
    constNum <- sum(constCol, na.rm = T)
    if (constNum > 0) {
      message(paste("After quantile normalization", constNum, 
                    "features with a constant value were found and deleted."))
      data <- data[, !constCol, drop = FALSE]
      colNames <- colnames(data)
      rowNames <- rownames(data)
    }
    rownm <- "Quantile Normalization"
  } else if (rowNorm == "GroupPQN") {
    grp_inx <- metadata_tab$group == ref
    ref.smpl <- apply(data[grp_inx, , drop = FALSE], 2, mean)
    data <- t(apply(data, 1, ProbNorm, ref.smpl))
    rownm <- "Probabilistic Quotient Normalization by a reference group"
  } else if (rowNorm == "SamplePQN") {
    ref.smpl <- data[ref, , drop = FALSE]
    data <- t(apply(data, 1, ProbNorm, ref.smpl))
    rownm <- "Probabilistic Quotient Normalization by a reference sample"
  } else if (rowNorm == "CompNorm") {
    data <- t(apply(t(data), 1, CompNorm, ref))
    rownm <- "Normalization by a reference feature";
  } else if (rowNorm == "SumNorm") {
    data <- t(apply(data, 1, SumNorm))
    rownm <- "Normalization to constant sum"
  } else if (rowNorm == "MedianNorm") {
    data <- t(apply(data, 1, MedianNorm))
    rownm <- "Normalization to sample median"
  } else if(rowNorm == "SpecNorm") {
    norm.vec <- rep(SpeWeight, nrow(data)) # default all same weight vec to prevent error
    data <- data / norm.vec
    message("No sample specific information were given, all set to 1.0")
    rownm <- "Normalization by sample-specific factor"
  } else {
    # nothing to do
    rownm <- "N/A"
  }
  ################################################ 
  
  # use apply will lose dimension info (i.e. row names and colnames)
  rownames(data) <- rowNames
  colnames(data) <- colNames
  
  # if the reference by feature, the feature column should be removed, since it is all 1
  if(rowNorm == "CompNorm" && !is.null(ref)){
    inx <- match(ref, colnames(data))
    data <- data[, -inx, drop=FALSE]
    colNames <- colNames[-inx]
  }
  
  #############################################
  #  Data transformation
  # generalize log, tolerant to 0 and negative values
  LogNorm <- function(x, min.val) {
    return(log10((x + sqrt(x^2 + min.val^2))/2))
  }
  
  # square root, tolerant to negative values
  SquareRootNorm <- function(x, min.val) {
    return(((x + sqrt(x^2 + min.val^2))/2)^(1/2))
  }
  
  if (transNorm == "LogNorm") {
    min.val <- min(abs(data[data != 0]))/10
    data <- apply(data, 2, LogNorm, min.val)
    transnm <- "Log10 Normalization"
  } else if (transNorm == "SrNorm") {
    min.val <- min(abs(data[data != 0]))/10
    data <- apply(data, 2, SquareRootNorm, min.val)
    transnm <- "Square Root Transformation"
  } else if (transNorm == "CrNorm") {
    norm.data <- abs(data)^(1/3)
    norm.data[data < 0] <- -norm.data[data < 0]
    data <- norm.data
    transnm <- "Cubic Root Transformation"
  } else {
    transnm <- "N/A"
  }
  #############################################
  
  #############################################
  # Data scaling
  # normalize to zero mean and unit variance
  AutoNorm <- function(x) {
    return((x - mean(x))/sd(x, na.rm = T))
  }
  
  # normalize to zero mean but variance/SE
  ParetoNorm <- function(x) {
    return((x - mean(x))/sqrt(sd(x, na.rm = T)))
  }
  
  # normalize to zero mean but variance/SE
  MeanCenter <- function(x) {
    return(x - mean(x))
  }
  
  # normalize to zero mean but variance/SE
  RangeNorm <- function(x) {
    if (max(x) == min(x)) {
      return(x)
    } else {
      return((x - mean(x))/(max(x) - min(x)))
    }
  }

  if (scaleNorm == "MeanCenter") {
    data <- apply(data, 2, MeanCenter)
    scalenm <- "Mean Centering"
  } else if (scaleNorm == "AutoNorm") {
    data <- apply(data, 2, AutoNorm)
    scalenm <- "Autoscaling"
  } else if (scaleNorm == "ParetoNorm") {
    data <- apply(data, 2, ParetoNorm)
    scalenm <- "Pareto Scaling"
  } else if (scaleNorm == "RangeNorm") {
    data <- apply(data, 2, RangeNorm)
    scalenm <- "Range Scaling"
  } else {
    scalenm <- "N/A"
  }
  ############################################# 
  
  message("Row norm: ", rownm, "\n",
          "Data Transformation norm: ", transnm, "\n",
          "Data Scaling norm: ", scalenm, "\n")  
  
  # note after using "apply" function, all the attribute lost, need to add back
  rownames(data) <- rowNames
  colnames(data) <- colNames
  
  target <- metadata_tab %>% 
    data.frame() %>%
    tibble::rownames_to_column("SampleID") %>%
    dplyr::filter(SampleID%in%rownames(data))
  se <- PomaSummarizedExperiment(target = target, 
                                 features = data)
  
  # need to do some sanity check, for log there may be Inf values introduced
  res <- CheckData(se)
  
  return(res)
}

se_normalize <- NormalizeData(
                      object = se_impute,
                      rowNorm = "None",
                      transNorm = "LogNorm",
                      scaleNorm = "ParetoNorm")

se_normalize
```

### Normalization by **POMA** R package
```{r}
none <- PomaNorm(se_impute, method = "none")
auto_scaling <- PomaNorm(se_impute, method = "auto_scaling")
evel_scaling <- PomaNorm(se_impute, method = "level_scaling")
log_scaling <- PomaNorm(se_impute, method = "log_scaling")
log_transformation <- PomaNorm(se_impute, method = "log_transformation")
vast_scaling <- PomaNorm(se_impute, method = "vast_scaling")
se_normalize_v2 <- PomaNorm(se_impute, method = "log_pareto")
se_normalize_v2
```

### Comparison of unnormalized and normalized dataset

* boxplot
```{r, fig.width=14, fig.height=12}
pl_unnor <- PomaBoxplots(se_impute, group = "samples", jitter = FALSE) +
  ggtitle("Not Normalized") +
  theme(legend.position = "none") # data before normalization

pl_nor <- PomaBoxplots(se_normalize, group = "samples", jitter = FALSE) +
  ggtitle("Normalized") # data after normalization

cowplot::plot_grid(pl_unnor, pl_nor, ncol = 1, align = "v")
```


* density
```{r, fig.width=10, fig.height=8}
pl_unnor <- PomaDensity(se_impute, group = "features") +
  ggtitle("Not Normalized") +
  theme(legend.position = "none") # data before normalization

pl_nor <- PomaDensity(se_normalize, group = "features") +
  ggtitle("Normalized") # data after normalization

cowplot::plot_grid(pl_unnor, pl_nor, ncol = 1, align = "v")
```

## Removing outliers
```{r, fig.width=10}
PomaOutliers(se_normalize, do = "analyze")$polygon_plot # to explore

se_processed <- PomaOutliers(se_normalize, do = "clean") # to remove outliers
se_processed
```


## Saving datasets into RDS files
```{r}
if (!dir.exists("./dataset/POMA/")) {
  dir.create("./dataset/POMA/")
}

saveRDS(se_filter, "./dataset/POMA/se_filter.RDS", compress = TRUE)
saveRDS(se_impute, "./dataset/POMA/se_impute.RDS", compress = TRUE)
saveRDS(se_normalize, "./dataset/POMA/se_normalize.RDS", compress = TRUE)
saveRDS(se_processed, "./dataset/POMA/se_processed.RDS", compress = TRUE)
```


## Systematic Information
```{r}
devtools::session_info()
```
