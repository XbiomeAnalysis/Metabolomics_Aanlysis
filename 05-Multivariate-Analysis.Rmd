# Multivariate analysis


Comparing to **univariate analysis**, **multivariate analysis** is defined as a process of involving multiple dependent variables resulting in one outcome for feature selection. Here, we use Regularized Generalized Linear Models and Random forest model to identify the biomarkers associated with Outcomes.

* Lasso, Ridge and Elasticnet Regularized Generalized Linear Models for Binary Outcomes

* Random forest model to select the important features

```{r}
knitr::opts_chunk$set(warning = F)

library(dplyr)
library(tibble)
library(POMA)
library(ggplot2)
library(ggraph)
library(plotly)
library(SummarizedExperiment)
library(glmnet)

# rm(list = ls())
options(stringsAsFactors = F)
options(future.globals.maxSize = 1000 * 1024^2)
```

## Importing data

The input data sets are from the previous chapter.
```{r}
se_processed <- readRDS("./dataset/POMA/se_processed.RDS")
```

## Data curation
```{r}
group_names <- c("Mild", "Severe")

se_processed_subset <- se_processed[, se_processed$group %in% group_names]
se_processed_subset$group <- factor(as.character(se_processed_subset$group))
```


## Regularized Generalized Linear Models

### Lasso: alpha = 1
```{r, fig.width=12, fig.height=6}
lasso_res <- PomaLasso(se_processed_subset, alpha = 1, labels = TRUE)

cowplot::plot_grid(lasso_res$cvLassoPlot,
                   lasso_res$coefficientPlot,
                   ncol = 2, align = "hv")

lasso_res$coefficients
```

### Ridge: alpha = 0
```{r, fig.width=12, fig.height=6}
ridge_res <- PomaLasso(se_processed_subset, alpha = 0, labels = TRUE)

cowplot::plot_grid(ridge_res$cvLassoPlot,
                   ridge_res$coefficientPlot,
                   ncol = 2, align = "hv")

ridge_res$coefficients
```

### Elasticnet: 0 < alpha < 1
```{r, fig.width=12, fig.height=6}
elastic_res <- PomaLasso(se_processed_subset, alpha = 0.4, labels = TRUE)

cowplot::plot_grid(elastic_res$cvLassoPlot,
                   elastic_res$coefficientPlot,
                   ncol = 2, align = "hv")

elastic_res$coefficients
```

## Classification

### Random Forest

* Calculation
```{r, fig.width=6, fig.height=4}
poma_rf <- PomaRandForest(se_processed_subset, ntest = 10, nvar = 10)
poma_rf$error_tree
```

* table
```{r}
poma_rf$confusionMatrix$table
```

* Important features
```{r, fig.width=6, fig.height=4}
poma_rf$MeanDecreaseGini_plot
```


### Support Vector Machine (SVM)


## Systematic Information
```{r}
devtools::session_info()
```

